# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-h5IgUVWO6X_9xPaLWcjRNTpz9KFJtiQ
"""

# =========================================================
# AI Helpdesk Retrieval Pipeline (Cleaned & Fixed)
# =========================================================

# 1Ô∏è‚É£ Install libraries (Colab)
# !pip install -q faiss-cpu sentence-transformers pymongo

# 2Ô∏è‚É£ Mount Google Drive (Colab)
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# 3Ô∏è‚É£ Imports
import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from pymongo import MongoClient
from datetime import datetime, timezone
import pickle
import os

# =========================================================
# 4Ô∏è‚É£ Paths
# =========================================================
CSV_PATH = "/content/drive/MyDrive/hr_dataset_modified.csv"
SAVE_DIR = "/content/drive/MyDrive/ai_helpdesk_files"
FAISS_PATH = os.path.join(SAVE_DIR, "faiss_index.bin")
DATA_PICKLE_PATH = os.path.join(SAVE_DIR, "data.pkl")
MODEL_PATH = "/content/drive/MyDrive/minilm-finetuned"  # Fine-tuned MiniLM

os.makedirs(SAVE_DIR, exist_ok=True)

# =========================================================
# 5Ô∏è‚É£ Load dataset
# =========================================================
df = pd.read_csv(CSV_PATH)
df.columns = ["Question", "Answer"]
df["Question"] = df["Question"].apply(lambda x: str(x).lower().strip())
questions = df["Question"].tolist()
answers = df["Answer"].tolist()

# =========================================================
# 6Ô∏è‚É£ Load MiniLM
# =========================================================
embedding_model = SentenceTransformer(MODEL_PATH)

# =========================================================
# 7Ô∏è‚É£ Build or load FAISS index and save data.pkl
# =========================================================
if os.path.exists(FAISS_PATH) and os.path.exists(DATA_PICKLE_PATH):
    print("Loading FAISS index and data.pkl from Drive...")
    index = faiss.read_index(FAISS_PATH)
    with open(DATA_PICKLE_PATH, "rb") as f:
        answers = pickle.load(f)
else:
    print("Building FAISS index from dataset...")
    embeddings = embedding_model.encode(questions, convert_to_numpy=True, show_progress_bar=True)
    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)
    index.add(embeddings)

    # Save to Drive
    faiss.write_index(index, FAISS_PATH)
    with open(DATA_PICKLE_PATH, "wb") as f:
        pickle.dump(answers, f)

    print(f"FAISS index saved at: {FAISS_PATH}")
    print(f"data.pkl saved at: {DATA_PICKLE_PATH}")

# =========================================================
# 8Ô∏è‚É£ MongoDB Setup (optional)
# =========================================================
MONGO_URL = "mongodb+srv://konainzehra:%23118343Ko@cluster0.djxrzfj.mongodb.net/?appName=Cluster0"
client = MongoClient(MONGO_URL)
db = client["ai_helpdesk"]
collection = db["helpdesk_logs"]

def store_log(question, answer, source="ai_helpdesk"):
    collection.insert_one({
        "question": question,
        "answer": answer,
        "source": source,
        "timestamp": datetime.now(timezone.utc)
    })

# =========================================================
# 9Ô∏è‚É£ Confidential Filter
# =========================================================
CONFIDENTIAL_KEYWORDS = ["password", "salary", "bank", "ssn", "credit card", "confidential"]

def is_confidential(query):
    return any(word in query.lower() for word in CONFIDENTIAL_KEYWORDS)

# =========================================================
# üîü Helpdesk Pipeline
# =========================================================
CONFIDENCE_THRESHOLD = 0.65

def helpdesk_pipeline(query, k=1, confidence_threshold=CONFIDENCE_THRESHOLD):
    # Confidential check
    if is_confidential(query):
        answer = "Access Restricted"
        store_log(query, answer, source="confidential_block")
        return answer

    # Encode query
    query_vec = embedding_model.encode([query], convert_to_numpy=True)
    query_vec = query_vec / np.linalg.norm(query_vec, axis=1, keepdims=True)

    # FAISS search
    D, I = index.search(query_vec, k)
    if I[0][0] == -1:
        answer = "Please visit the official website or contact HR department at (051) 5951821."
        store_log(query, answer, source="fallback")
        return answer

    similarity = float(D[0][0])
    answer_text = answers[I[0][0]]

    print("Similarity Score:", similarity)

    # Confidence threshold check
    if similarity < confidence_threshold:
        answer = "Please visit the official website or contact HR department at (051) 5951821."
        store_log(query, answer, source="fallback")
        return answer

    store_log(query, answer_text, source="retrieval")
    return answer_text

# =========================================================
# 1Ô∏è‚É£1Ô∏è‚É£ Interactive Query Loop
# =========================================================
if __name__ == "__main__":
    print("ü§ñ AI Help Desk (type 'exit' to quit)\n")
    while True:
        user_query = input("Ask your question: ").strip()
        if user_query.lower() in ["exit", "quit"]:
            print("Goodbye üëã")
            break
        response = helpdesk_pipeline(user_query)
        print("Answer:", response)
        print("-" * 50)